import praw # https://praw.readthedocs.io/en/latest/import osfrom dotenv import load_dotenvimport nltkimport re#grab .envload_dotenv()# load in praw through .envreddit = praw.Reddit(    client_id    = os.getenv("CLIENT_ID"),    client_secret= os.getenv("CLIENT_SECRET"),    username     = os.getenv("USERNAME"),    password     = os.getenv("PASSWORD"),    user_agent   = os.getenv("USER_AGENT"),    check_for_async=False)# check for successprint(reddit.user.me())# load subreddit instancessubreddit = reddit.subreddit('Wallstreetbetsnew')# sorting the posts / how many amounts to scrape# check submissions class in documentation for more infosubmissions = list(subreddit.hot(limit=10)) # must be a list so we can iterate through it multiple timesfor submission in submissions:    print(f"submission.title: {submission.title}")# tokenize the words in the titleword_collection=[]for submission in submissions:    title_words = nltk.word_tokenize(submission.title,language="english")    word_collection.extend(title_words)    # find out if a token is a tickerticker_pattern = re.compile(r'^[A-Z]{4}$')potential_tickers = []for word in word_collection:    if ticker_pattern.match(word):        potential_tickers.append(word)# TO DO # With potential_tickers, get a sentiment score for sentences that include them